{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimize GWR bandwidth using Reinforcement learning approach (Proximal Policy Optimization, PPO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "import pandas as pd\n",
    "\n",
    "from src.optimizer.reinforce.gwr_optimizer import GwrOptimizerRL\n",
    "from src.dataset.interfaces.spatial_dataset import IFieldInfo\n",
    "from src.optimizer.reinforce.callback import EpisodeTracker\n",
    "from src.dataset.spatial_dataset import SpatialDataset\n",
    "from src.kernel.gwr_kernel import GwrKernel\n",
    "from src.log.gwr_logger import GwrLogger\n",
    "from src.model.gwr import GWR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a logger to record the GWR model's information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = GwrLogger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Georgia dataset and create a spatial dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'2025-03-15 04:58:04': 'SpatialDataset : Data schema matchs with the data.'}\n",
      "{'2025-03-15 04:58:04': 'SpatialDataset : Data points created.'}\n"
     ]
    }
   ],
   "source": [
    "georgia_data = pd.read_csv(r'./data/GData_utm.csv')\n",
    "spatialDataset = SpatialDataset(\n",
    "\tgeorgia_data,\n",
    "\tIFieldInfo(\n",
    "\t\tpredictor_fields=['PctFB', 'PctBlack', 'PctRural'],\n",
    "\t\tresponse_field='PctBach',\n",
    "\t\tcoordinate_x_field='Longitud',\n",
    "\t\tcoordinate_y_field='Latitude'\n",
    "\t),\n",
    "\tlogger,\n",
    "\tisSpherical=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a GWR kernel and GWR model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'2025-03-15 04:58:04': 'GwrKernel : Kernel is initialized.'}\n",
      "{'2025-03-15 04:58:04': 'GWR : GWR model is initialized.'}\n"
     ]
    }
   ],
   "source": [
    "kernel = GwrKernel(\n",
    "\tspatialDataset,\n",
    "\tlogger,\n",
    "\tkernel_type='bisquare',\n",
    "\tkernel_bandwidth_type='adaptive'\n",
    ")\n",
    "gwr = GWR(spatialDataset, kernel, logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize gwr gym environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'2025-03-15 04:58:04': 'GwrOptimizerRL: GwrOptimizerRL environment is initialized.'}\n"
     ]
    }
   ],
   "source": [
    "env = GwrOptimizerRL(\n",
    "\tgwr,\n",
    "\tlogger,\n",
    "\tmin_bandwidth=10,\n",
    "\tmax_bandwidth=spatialDataset.x_matrix.shape[0],\n",
    "\tmin_action=-10,\n",
    "\tmax_action=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using PPO to optimize the bandwidth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "{'2025-03-15 05:01:19': 'Episode 1 ends, total steps: 100, remaining steps: 4900'}\n",
      "{'2025-03-15 05:01:21': 'Episode 2 ends, total steps: 100, remaining steps: 4800'}\n",
      "{'2025-03-15 05:01:22': 'Episode 3 ends, total steps: 100, remaining steps: 4700'}\n",
      "{'2025-03-15 05:01:24': 'Episode 4 ends, total steps: 100, remaining steps: 4600'}\n",
      "{'2025-03-15 05:01:24': 'episode 90 reached the reward threshold: R2 0.7629490710812805, bandwidth: 36'}\n",
      "{'2025-03-15 05:01:24': 'Episode 5 ends, total steps: 1, remaining steps: 4599'}\n",
      "{'2025-03-15 05:01:24': 'episode 91 reached the reward threshold: R2 0.8255832052225135, bandwidth: 24'}\n",
      "{'2025-03-15 05:01:24': 'Episode 6 ends, total steps: 1, remaining steps: 4598'}\n",
      "{'2025-03-15 05:01:26': 'Episode 7 ends, total steps: 100, remaining steps: 4498'}\n",
      "{'2025-03-15 05:01:28': 'Episode 8 ends, total steps: 100, remaining steps: 4398'}\n",
      "{'2025-03-15 05:01:30': 'Episode 9 ends, total steps: 100, remaining steps: 4298'}\n",
      "{'2025-03-15 05:01:32': 'Episode 10 ends, total steps: 100, remaining steps: 4198'}\n",
      "{'2025-03-15 05:01:33': 'Episode 11 ends, total steps: 100, remaining steps: 4098'}\n",
      "{'2025-03-15 05:01:35': 'Episode 12 ends, total steps: 100, remaining steps: 3998'}\n",
      "{'2025-03-15 05:01:38': 'Episode 13 ends, total steps: 100, remaining steps: 3898'}\n",
      "{'2025-03-15 05:01:40': 'Episode 14 ends, total steps: 100, remaining steps: 3798'}\n",
      "{'2025-03-15 05:01:42': 'Episode 15 ends, total steps: 100, remaining steps: 3698'}\n",
      "{'2025-03-15 05:01:44': 'Episode 16 ends, total steps: 100, remaining steps: 3598'}\n",
      "{'2025-03-15 05:01:46': 'Episode 17 ends, total steps: 100, remaining steps: 3498'}\n",
      "{'2025-03-15 05:01:48': 'Episode 18 ends, total steps: 100, remaining steps: 3398'}\n",
      "{'2025-03-15 05:01:48': 'episode 104 reached the reward threshold: R2 0.7864509020757883, bandwidth: 30'}\n",
      "{'2025-03-15 05:01:48': 'Episode 19 ends, total steps: 1, remaining steps: 3397'}\n",
      "{'2025-03-15 05:01:50': 'Episode 20 ends, total steps: 100, remaining steps: 3297'}\n",
      "{'2025-03-15 05:01:52': 'Episode 21 ends, total steps: 100, remaining steps: 3197'}\n",
      "{'2025-03-15 05:01:52': 'episode 107 reached the reward threshold: R2 0.9281512805191626, bandwidth: 12'}\n",
      "{'2025-03-15 05:01:52': 'Episode 22 ends, total steps: 1, remaining steps: 3196'}\n",
      "{'2025-03-15 05:01:54': 'Episode 23 ends, total steps: 100, remaining steps: 3096'}\n",
      "{'2025-03-15 05:01:57': 'Episode 24 ends, total steps: 100, remaining steps: 2996'}\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 83.5     |\n",
      "|    ep_rew_mean     | 56.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 49       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 41       |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "{'2025-03-15 05:02:00': 'Episode 25 ends, total steps: 100, remaining steps: 2896'}\n",
      "{'2025-03-15 05:02:02': 'Episode 26 ends, total steps: 100, remaining steps: 2796'}\n",
      "{'2025-03-15 05:02:04': 'Episode 27 ends, total steps: 100, remaining steps: 2696'}\n",
      "{'2025-03-15 05:02:06': 'Episode 28 ends, total steps: 100, remaining steps: 2596'}\n",
      "{'2025-03-15 05:02:08': 'Episode 29 ends, total steps: 100, remaining steps: 2496'}\n",
      "{'2025-03-15 05:02:10': 'Episode 30 ends, total steps: 100, remaining steps: 2396'}\n",
      "{'2025-03-15 05:02:12': 'Episode 31 ends, total steps: 100, remaining steps: 2296'}\n",
      "{'2025-03-15 05:02:14': 'Episode 32 ends, total steps: 100, remaining steps: 2196'}\n",
      "{'2025-03-15 05:02:16': 'Episode 33 ends, total steps: 100, remaining steps: 2096'}\n",
      "{'2025-03-15 05:02:16': 'episode 119 reached the reward threshold: R2 0.9550037242036704, bandwidth: 10'}\n",
      "{'2025-03-15 05:02:16': 'Episode 34 ends, total steps: 1, remaining steps: 2095'}\n",
      "{'2025-03-15 05:02:18': 'Episode 35 ends, total steps: 100, remaining steps: 1995'}\n",
      "{'2025-03-15 05:02:20': 'Episode 36 ends, total steps: 100, remaining steps: 1895'}\n",
      "{'2025-03-15 05:02:22': 'Episode 37 ends, total steps: 100, remaining steps: 1795'}\n",
      "{'2025-03-15 05:02:24': 'Episode 38 ends, total steps: 100, remaining steps: 1695'}\n",
      "{'2025-03-15 05:02:26': 'Episode 39 ends, total steps: 100, remaining steps: 1595'}\n",
      "{'2025-03-15 05:02:28': 'Episode 40 ends, total steps: 100, remaining steps: 1495'}\n",
      "{'2025-03-15 05:02:30': 'Episode 41 ends, total steps: 100, remaining steps: 1395'}\n",
      "{'2025-03-15 05:02:32': 'Episode 42 ends, total steps: 100, remaining steps: 1295'}\n",
      "{'2025-03-15 05:02:35': 'Episode 43 ends, total steps: 100, remaining steps: 1195'}\n",
      "{'2025-03-15 05:02:35': 'episode 129 reached the reward threshold: R2 0.8191649140960532, bandwidth: 25'}\n",
      "{'2025-03-15 05:02:35': 'Episode 44 ends, total steps: 1, remaining steps: 1194'}\n",
      "{'2025-03-15 05:02:37': 'Episode 45 ends, total steps: 100, remaining steps: 1094'}\n",
      "{'2025-03-15 05:02:39': 'Episode 46 ends, total steps: 100, remaining steps: 994'}\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 87.1         |\n",
      "|    ep_rew_mean          | 59.2         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 48           |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 83           |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061737797 |\n",
      "|    clip_fraction        | 0.0475       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | -0.00115     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.19         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00236     |\n",
      "|    std                  | 0.987        |\n",
      "|    value_loss           | 19.1         |\n",
      "------------------------------------------\n",
      "{'2025-03-15 05:02:42': 'Episode 47 ends, total steps: 100, remaining steps: 894'}\n",
      "{'2025-03-15 05:02:44': 'Episode 48 ends, total steps: 100, remaining steps: 794'}\n",
      "{'2025-03-15 05:02:46': 'Episode 49 ends, total steps: 100, remaining steps: 694'}\n",
      "{'2025-03-15 05:02:46': 'episode 135 reached the reward threshold: R2 0.8400920454252443, bandwidth: 22'}\n",
      "{'2025-03-15 05:02:46': 'Episode 50 ends, total steps: 1, remaining steps: 693'}\n",
      "{'2025-03-15 05:02:46': 'episode 136 reached the reward threshold: R2 0.8400920454252443, bandwidth: 22'}\n",
      "{'2025-03-15 05:02:46': 'Episode 51 ends, total steps: 1, remaining steps: 692'}\n",
      "{'2025-03-15 05:02:47': 'episode 137 reached the reward threshold: R2 0.7512236959466968, bandwidth: 40'}\n",
      "{'2025-03-15 05:02:47': 'Episode 52 ends, total steps: 46, remaining steps: 646'}\n",
      "{'2025-03-15 05:02:48': 'episode 138 reached the reward threshold: R2 0.7512236959466968, bandwidth: 40'}\n",
      "{'2025-03-15 05:02:48': 'Episode 53 ends, total steps: 61, remaining steps: 585'}\n",
      "{'2025-03-15 05:02:48': 'episode 139 reached the reward threshold: R2 0.7926896433004649, bandwidth: 29'}\n",
      "{'2025-03-15 05:02:48': 'Episode 54 ends, total steps: 1, remaining steps: 584'}\n",
      "{'2025-03-15 05:02:50': 'Episode 55 ends, total steps: 100, remaining steps: 484'}\n",
      "{'2025-03-15 05:02:52': 'Episode 56 ends, total steps: 100, remaining steps: 384'}\n",
      "{'2025-03-15 05:02:54': 'Episode 57 ends, total steps: 100, remaining steps: 284'}\n",
      "{'2025-03-15 05:02:56': 'Episode 58 ends, total steps: 100, remaining steps: 184'}\n",
      "{'2025-03-15 05:02:56': 'episode 144 reached the reward threshold: R2 0.766316825518426, bandwidth: 35'}\n",
      "{'2025-03-15 05:02:56': 'Episode 59 ends, total steps: 1, remaining steps: 183'}\n",
      "{'2025-03-15 05:02:57': 'episode 145 reached the reward threshold: R2 0.7512236959466968, bandwidth: 40'}\n",
      "{'2025-03-15 05:02:57': 'Episode 60 ends, total steps: 81, remaining steps: 102'}\n",
      "{'2025-03-15 05:02:59': 'Episode 61 ends, total steps: 100, remaining steps: 2'}\n",
      "{'2025-03-15 05:03:01': 'Episode 62 ends, total steps: 100, remaining steps: -98'}\n",
      "{'2025-03-15 05:03:03': 'episode 148 reached the reward threshold: R2 0.7512236959466968, bandwidth: 40'}\n",
      "{'2025-03-15 05:03:03': 'Episode 63 ends, total steps: 94, remaining steps: -192'}\n",
      "{'2025-03-15 05:03:06': 'Episode 64 ends, total steps: 100, remaining steps: -292'}\n",
      "{'2025-03-15 05:03:08': 'Episode 65 ends, total steps: 100, remaining steps: -392'}\n",
      "{'2025-03-15 05:03:10': 'Episode 66 ends, total steps: 100, remaining steps: -492'}\n",
      "{'2025-03-15 05:03:12': 'Episode 67 ends, total steps: 100, remaining steps: -592'}\n",
      "{'2025-03-15 05:03:14': 'Episode 68 ends, total steps: 100, remaining steps: -692'}\n",
      "{'2025-03-15 05:03:14': 'episode 154 reached the reward threshold: R2 0.7544607566118469, bandwidth: 39'}\n",
      "{'2025-03-15 05:03:14': 'Episode 69 ends, total steps: 6, remaining steps: -698'}\n",
      "{'2025-03-15 05:03:16': 'Episode 70 ends, total steps: 100, remaining steps: -798'}\n",
      "{'2025-03-15 05:03:19': 'Episode 71 ends, total steps: 100, remaining steps: -898'}\n",
      "{'2025-03-15 05:03:21': 'Episode 72 ends, total steps: 100, remaining steps: -998'}\n",
      "{'2025-03-15 05:03:21': 'episode 158 reached the reward threshold: R2 0.8400920454252443, bandwidth: 22'}\n",
      "{'2025-03-15 05:03:21': 'Episode 73 ends, total steps: 1, remaining steps: -999'}\n",
      "{'2025-03-15 05:03:22': 'episode 159 reached the reward threshold: R2 0.7512236959466968, bandwidth: 40'}\n",
      "{'2025-03-15 05:03:22': 'Episode 74 ends, total steps: 82, remaining steps: -1081'}\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 82.2        |\n",
      "|    ep_rew_mean          | 56.4        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 48          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 127         |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004034877 |\n",
      "|    clip_fraction        | 0.012       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 0.00247     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.08        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00168    |\n",
      "|    std                  | 0.986       |\n",
      "|    value_loss           | 32.1        |\n",
      "-----------------------------------------\n",
      "{'2025-03-15 05:03:25': 'PPO: PPO finished training.'}\n"
     ]
    }
   ],
   "source": [
    "TOTAL_TIMESTEPS = 5000\n",
    "episodeTracker = EpisodeTracker(\n",
    "  logger,\n",
    "  total_timesteps=TOTAL_TIMESTEPS\n",
    ")\n",
    "model = PPO(\n",
    "  \"MlpPolicy\", \n",
    "  env, \n",
    "  verbose=1, \n",
    "  device='cpu'\n",
    ")\n",
    "model.learn(\n",
    "  total_timesteps=TOTAL_TIMESTEPS, \n",
    "  callback=episodeTracker\n",
    ")\n",
    "logger.append_info(\"PPO: PPO finished training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, _ = env.reset()\n",
    "for _ in range(100):\n",
    "\taction, _ = model.predict(obs)\n",
    "\tobs, reward, done, truncated, _ = env.step(action)\n",
    "\tlogger.append_info(\n",
    "\t\tf\"Bandwidth: {obs}, Reward (R2): {reward}\"\n",
    "\t)\n",
    "\tif done or truncated:\n",
    "\t\tbreak\n",
    "\n",
    "logger.save_model_info_json()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lmgwr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
