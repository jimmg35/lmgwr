{
 "model_type": "lgwr",
 "bandwidth_optimization": [
  {
   "episode": 3,
   "reward": 299.5188741608224,
   "r2": 0.6843304994193867,
   "bandwidth": "[103, 102, 106, 105, 105, 105, 103, 104, 102, 102, 103, 103, 103, 103, 104, 105, 103, 105, 104, 104, 106, 105, 104, 103, 103, 104, 103, 104, 103, 106, 104, 102, 104, 106, 105, 104, 104, 103, 105, 105, 104, 105, 103, 104, 105, 105, 106, 104, 106, 103, 106, 104, 103, 105, 105, 105, 104, 103, 106, 103, 103, 104, 105, 106, 104, 106, 102, 105, 104, 103, 104, 104, 104, 104, 105, 103, 105, 105, 105, 105, 103, 104, 104, 104, 104, 103, 103, 104, 103, 103, 104, 102, 105, 104, 104, 103, 104, 103, 105, 103, 104, 102, 103, 106, 102, 104, 103, 104, 102, 103, 103, 104, 104, 105, 103, 102, 104, 103, 105, 103, 104, 104, 104, 104, 105, 104, 105, 104, 104, 104, 104, 102, 104, 103, 103, 104, 105, 103, 104, 103, 104, 106, 105, 105, 105, 104, 105, 104, 105, 103, 104, 105, 104, 103, 103, 103, 104, 105, 103]"
  },
  {
   "episode": 6,
   "reward": 299.7054626436889,
   "r2": 0.6843404237435455,
   "bandwidth": "[104, 102, 104, 102, 103, 103, 102, 103, 101, 103, 103, 102, 104, 103, 103, 103, 103, 104, 103, 104, 103, 103, 102, 104, 104, 104, 101, 103, 102, 103, 103, 102, 104, 104, 103, 103, 103, 103, 102, 104, 103, 103, 104, 102, 105, 104, 103, 104, 105, 102, 105, 103, 104, 103, 103, 104, 103, 103, 105, 103, 105, 104, 104, 102, 104, 103, 103, 102, 105, 102, 103, 105, 102, 102, 102, 104, 105, 102, 104, 103, 102, 103, 103, 103, 101, 104, 102, 103, 103, 104, 103, 103, 103, 104, 103, 101, 102, 101, 102, 102, 103, 104, 105, 102, 103, 104, 102, 103, 104, 103, 102, 104, 105, 104, 102, 103, 104, 103, 103, 104, 104, 101, 103, 102, 103, 102, 105, 102, 105, 104, 102, 104, 101, 104, 103, 102, 103, 105, 103, 101, 103, 104, 103, 103, 101, 105, 104, 104, 104, 102, 103, 101, 103, 104, 104, 102, 104, 102, 103]"
  },
  {
   "episode": 7,
   "reward": 299.76669279057717,
   "r2": 0.6685862215098575,
   "bandwidth": "[131, 131, 131, 129, 127, 133, 131, 130, 130, 132, 131, 133, 131, 132, 133, 131, 132, 130, 129, 131, 131, 130, 132, 129, 129, 133, 130, 130, 133, 131, 134, 128, 130, 131, 131, 129, 130, 131, 133, 130, 134, 131, 133, 132, 129, 133, 131, 130, 133, 130, 130, 131, 131, 128, 131, 132, 133, 132, 131, 128, 128, 129, 133, 127, 130, 132, 129, 132, 133, 135, 130, 132, 131, 129, 131, 129, 130, 131, 129, 131, 129, 129, 132, 130, 129, 129, 127, 131, 127, 130, 130, 132, 133, 130, 132, 132, 134, 135, 134, 132, 133, 129, 130, 130, 132, 131, 133, 131, 132, 130, 131, 131, 129, 129, 130, 132, 131, 128, 133, 130, 133, 130, 134, 132, 131, 131, 130, 134, 131, 129, 130, 129, 133, 129, 129, 133, 131, 131, 134, 131, 131, 132, 133, 128, 130, 130, 129, 130, 132, 131, 130, 130, 131, 131, 129, 133, 134, 131, 133]"
  },
  {
   "episode": 8,
   "reward": 299.949946131443,
   "r2": 0.6677982297503928,
   "bandwidth": "[128, 133, 129, 132, 133, 132, 132, 131, 130, 134, 132, 130, 133, 131, 133, 131, 130, 132, 135, 134, 134, 134, 132, 135, 132, 131, 132, 133, 132, 130, 135, 134, 129, 129, 128, 128, 130, 134, 135, 131, 134, 133, 135, 133, 132, 129, 132, 132, 132, 129, 131, 134, 131, 131, 131, 135, 133, 133, 130, 132, 131, 131, 132, 129, 135, 131, 132, 130, 131, 130, 132, 133, 132, 131, 133, 134, 135, 132, 131, 138, 132, 134, 131, 133, 133, 130, 132, 135, 132, 133, 128, 134, 131, 130, 130, 134, 132, 132, 132, 130, 133, 131, 130, 133, 127, 132, 135, 130, 128, 130, 130, 130, 131, 133, 134, 131, 131, 132, 136, 129, 133, 131, 131, 129, 133, 132, 133, 131, 129, 134, 131, 132, 130, 132, 132, 128, 130, 130, 134, 131, 131, 130, 132, 131, 132, 131, 130, 131, 133, 132, 134, 132, 130, 133, 131, 130, 134, 133, 133]"
  },
  {
   "episode": 9,
   "reward": 299.3148316383267,
   "r2": 0.6757787743344204,
   "bandwidth": "[119, 118, 119, 120, 119, 119, 119, 119, 118, 119, 120, 117, 119, 120, 122, 118, 120, 117, 119, 118, 121, 119, 120, 119, 120, 119, 117, 119, 119, 119, 118, 120, 120, 119, 119, 119, 119, 119, 119, 118, 119, 120, 119, 118, 118, 120, 118, 118, 118, 120, 120, 118, 120, 118, 117, 118, 119, 119, 120, 120, 118, 119, 120, 120, 119, 118, 120, 118, 120, 119, 120, 120, 119, 119, 120, 120, 120, 119, 120, 120, 120, 118, 119, 118, 118, 119, 119, 120, 118, 120, 119, 119, 117, 120, 118, 118, 119, 119, 119, 120, 119, 118, 118, 120, 118, 119, 120, 118, 120, 118, 118, 119, 120, 120, 120, 118, 119, 118, 120, 118, 119, 119, 119, 121, 118, 120, 119, 122, 119, 119, 118, 119, 119, 118, 120, 119, 119, 118, 120, 118, 116, 118, 118, 119, 119, 118, 119, 120, 119, 120, 120, 122, 119, 120, 120, 121, 119, 119, 119]"
  },
  {
   "episode": 11,
   "reward": 299.9207996619864,
   "r2": 0.6698971205462176,
   "bandwidth": "[100, 129, 119, 152, 150, 152, 100, 112, 96, 121, 127, 143, 138, 153, 153, 142, 147, 153, 109, 154, 153, 159, 126, 150, 158, 129, 133, 128, 136, 117, 104, 61, 147, 70, 75, 152, 150, 154, 154, 159, 148, 118, 128, 132, 154, 134, 90, 136, 156, 145, 145, 156, 143, 118, 156, 141, 153, 158, 139, 131, 157, 68, 153, 88, 105, 158, 112, 75, 159, 121, 136, 134, 117, 66, 152, 147, 98, 132, 135, 94, 131, 150, 156, 144, 82, 122, 101, 151, 84, 153, 90, 99, 127, 158, 150, 130, 121, 145, 110, 155, 125, 131, 140, 146, 141, 142, 107, 86, 141, 141, 157, 112, 112, 136, 134, 143, 152, 154, 154, 112, 145, 106, 128, 151, 150, 136, 128, 124, 148, 98, 137, 132, 127, 144, 159, 159, 136, 152, 127, 133, 89, 144, 110, 129, 153, 138, 140, 140, 145, 143, 144, 111, 97, 111, 134, 122, 147, 135, 157]"
  }
 ],
 "info": [
  {
   "2025-03-20 07:23:57": "SpatialDataset : Data schema matchs with the data."
  },
  {
   "2025-03-20 07:23:57": "SpatialDataset : Data points created."
  },
  {
   "2025-03-20 07:23:57": "LgwrKernel : Kernel is initialized."
  },
  {
   "2025-03-20 07:23:57": "LGWR : LGWR model is initialized."
  },
  {
   "2025-03-20 07:23:57": "LgwrOptimizerRL: LgwrOptimizerRL environment is initialized."
  },
  {
   "2025-03-20 07:23:57": "LgwrOptimizerRL: Using AICC as the reward."
  },
  {
   "2025-03-20 07:24:22": "Episode 1 truncated, took 1000 steps, remain 9000 steps, reward: -316.772119893014, r2: 0.7225659562765592."
  },
  {
   "2025-03-20 07:24:43": "Episode 2 truncated, took 1000 steps, remain 8000 steps, reward: -305.88318013899334, r2: 0.6866085132880994."
  },
  {
   "2025-03-20 07:25:07": "Episode 4 truncated, took 1000 steps, remain 6999 steps, reward: -313.4656704586282, r2: 0.7280213754971137."
  },
  {
   "2025-03-20 07:25:27": "Episode 5 truncated, took 1000 steps, remain 5999 steps, reward: -316.6151189139627, r2: 0.7088506326743338."
  },
  {
   "2025-03-20 07:25:49": "Episode 10 truncated, took 1000 steps, remain 4991 steps, reward: -307.5182893610279, r2: 0.6598032880790701."
  },
  {
   "2025-03-20 07:26:33": "Episode 12 truncated, took 1000 steps, remain 3084 steps, reward: -306.85501302255216, r2: 0.7484286240642518."
  },
  {
   "2025-03-20 07:26:54": "Episode 13 truncated, took 1000 steps, remain 2084 steps, reward: -304.6894132332387, r2: 0.7093116613056403."
  },
  {
   "2025-03-20 07:27:15": "Episode 14 truncated, took 1000 steps, remain 1084 steps, reward: -313.4702277846806, r2: 0.715193072391132."
  },
  {
   "2025-03-20 07:27:37": "Episode 15 truncated, took 1000 steps, remain 84 steps, reward: -303.0595247558541, r2: 0.6669310144674316."
  },
  {
   "2025-03-20 07:27:49": "PPO: PPO finished training."
  }
 ],
 "matrices": {
  "AIC": null,
  "AICc": null,
  "R-squared": null,
  "R-squared adjusted": null
 }
}