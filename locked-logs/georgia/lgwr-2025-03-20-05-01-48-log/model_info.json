{
 "model_type": "lgwr",
 "TOTAL_TIMESTEPS": 10000,
 "MAX_STEPS": 10,
 "bandwidth_optimization": [
  {
   "episode": 2,
   "reward": 299.4428788801515,
   "r2": 0.6824529860810862,
   "bandwidth": "[107, 108, 107, 107, 107, 107, 108, 108, 108, 108, 106, 106, 107, 106, 109, 106, 106, 107, 106, 108, 107, 107, 108, 106, 106, 107, 106, 106, 109, 109, 108, 109, 107, 106, 107, 107, 107, 108, 106, 110, 108, 108, 108, 109, 108, 107, 106, 108, 109, 109, 106, 105, 108, 108, 108, 108, 107, 106, 106, 108, 106, 107, 107, 108, 107, 107, 106, 107, 107, 105, 106, 107, 108, 109, 107, 109, 107, 108, 107, 107, 108, 107, 108, 105, 109, 106, 104, 107, 106, 106, 106, 107, 104, 107, 107, 107, 107, 108, 107, 106, 105, 108, 108, 108, 107, 107, 107, 107, 108, 107, 108, 107, 108, 106, 108, 109, 108, 107, 107, 106, 106, 107, 106, 107, 105, 107, 106, 106, 108, 108, 107, 107, 106, 107, 108, 107, 106, 108, 108, 108, 108, 108, 108, 108, 107, 109, 106, 109, 108, 105, 108, 106, 107, 106, 107, 107, 108, 108, 105]"
  },
  {
   "episode": 3,
   "reward": 299.88397859172187,
   "r2": 0.7144083570114239,
   "bandwidth": "[96, 75, 72, 67, 69, 58, 81, 65, 52, 96, 89, 59, 77, 73, 72, 52, 83, 87, 74, 74, 92, 55, 105, 84, 77, 95, 73, 53, 34, 94, 77, 95, 69, 83, 77, 82, 55, 81, 62, 76, 84, 73, 45, 66, 88, 90, 62, 90, 79, 75, 62, 75, 87, 48, 61, 48, 70, 88, 74, 87, 62, 90, 101, 80, 75, 91, 38, 67, 48, 87, 64, 69, 98, 59, 105, 66, 95, 80, 49, 100, 79, 83, 60, 88, 60, 69, 63, 64, 68, 70, 58, 46, 95, 83, 79, 84, 105, 61, 84, 96, 73, 113, 97, 78, 59, 94, 57, 78, 94, 45, 89, 89, 101, 78, 43, 83, 86, 88, 76, 73, 63, 72, 83, 54, 113, 94, 36, 80, 83, 102, 98, 111, 110, 83, 84, 76, 67, 67, 84, 63, 46, 81, 65, 105, 90, 79, 71, 77, 95, 68, 59, 74, 84, 71, 100, 83, 81, 67, 66]"
  },
  {
   "episode": 5,
   "reward": 299.9417450526659,
   "r2": 0.6682061590512485,
   "bandwidth": "[123, 133, 122, 138, 112, 139, 127, 124, 125, 130, 135, 156, 129, 130, 125, 117, 128, 132, 136, 156, 148, 143, 135, 142, 125, 126, 124, 135, 127, 146, 132, 137, 137, 138, 131, 135, 126, 136, 116, 131, 132, 125, 130, 136, 129, 136, 129, 126, 142, 124, 144, 126, 135, 121, 114, 127, 121, 140, 132, 147, 128, 116, 136, 130, 124, 128, 120, 132, 154, 131, 129, 126, 122, 125, 130, 132, 133, 127, 143, 122, 146, 132, 131, 158, 121, 148, 144, 138, 131, 152, 115, 140, 129, 114, 134, 117, 136, 116, 129, 141, 129, 130, 128, 155, 120, 123, 126, 153, 113, 122, 137, 133, 130, 122, 116, 120, 147, 101, 125, 158, 117, 138, 134, 139, 133, 130, 145, 134, 124, 115, 148, 137, 144, 117, 124, 136, 136, 129, 134, 110, 126, 141, 156, 146, 128, 147, 148, 131, 130, 120, 125, 142, 131, 114, 125, 118, 131, 147, 132]"
  },
  {
   "episode": 7,
   "reward": 299.8122059166476,
   "r2": 0.6944128771626747,
   "bandwidth": "[131, 107, 91, 99, 102, 107, 93, 98, 95, 85, 75, 87, 101, 124, 59, 136, 79, 130, 89, 117, 102, 96, 93, 98, 107, 82, 80, 94, 106, 102, 95, 120, 91, 89, 90, 117, 116, 124, 94, 107, 77, 66, 137, 73, 129, 84, 120, 81, 102, 65, 96, 62, 79, 58, 93, 89, 130, 87, 80, 90, 105, 118, 62, 79, 127, 91, 81, 105, 93, 104, 112, 76, 85, 114, 68, 120, 80, 120, 158, 68, 89, 117, 92, 83, 117, 100, 66, 94, 75, 88, 77, 94, 96, 102, 107, 92, 77, 104, 94, 91, 92, 134, 74, 73, 66, 54, 121, 99, 34, 98, 97, 87, 97, 106, 95, 93, 80, 95, 81, 78, 116, 110, 57, 106, 99, 76, 77, 117, 98, 95, 82, 65, 60, 97, 104, 57, 97, 99, 128, 67, 107, 88, 100, 106, 111, 90, 108, 69, 76, 89, 102, 67, 75, 101, 108, 68, 82, 127, 127]"
  },
  {
   "episode": 10,
   "reward": 299.0557538198806,
   "r2": 0.6784527820138593,
   "bandwidth": "[115, 115, 114, 115, 114, 113, 113, 115, 115, 115, 116, 116, 114, 116, 116, 114, 115, 117, 115, 116, 114, 114, 115, 112, 115, 115, 115, 117, 113, 117, 114, 115, 116, 114, 115, 116, 115, 115, 117, 115, 112, 116, 113, 115, 117, 116, 115, 117, 114, 115, 115, 116, 116, 115, 114, 116, 114, 113, 115, 113, 116, 115, 115, 114, 115, 116, 115, 115, 115, 114, 113, 115, 113, 116, 117, 114, 115, 115, 114, 115, 115, 114, 117, 116, 116, 115, 114, 114, 115, 116, 116, 115, 115, 117, 114, 115, 115, 115, 114, 117, 116, 116, 116, 116, 116, 115, 114, 115, 115, 116, 117, 114, 115, 116, 114, 114, 113, 114, 114, 116, 116, 116, 117, 118, 114, 115, 115, 115, 114, 115, 114, 115, 115, 116, 115, 117, 115, 113, 114, 116, 117, 113, 114, 114, 116, 114, 115, 115, 117, 115, 116, 114, 115, 116, 113, 115, 114, 115, 115]"
  },
  {
   "episode": 15,
   "reward": 299.5891400355884,
   "r2": 0.6828674185334039,
   "bandwidth": "[105, 106, 105, 105, 104, 107, 105, 106, 107, 104, 105, 107, 106, 107, 107, 105, 106, 106, 107, 107, 106, 106, 104, 106, 107, 107, 107, 106, 106, 106, 106, 109, 106, 107, 105, 106, 107, 105, 106, 104, 107, 106, 107, 106, 106, 107, 106, 108, 106, 105, 105, 106, 106, 107, 108, 105, 107, 104, 106, 106, 108, 107, 106, 106, 106, 107, 106, 106, 106, 106, 107, 106, 104, 106, 105, 106, 105, 107, 107, 105, 105, 105, 106, 104, 107, 106, 105, 106, 106, 106, 106, 104, 105, 105, 105, 106, 104, 107, 107, 106, 106, 106, 107, 108, 108, 105, 105, 106, 106, 107, 106, 105, 106, 105, 106, 105, 108, 106, 104, 107, 105, 107, 105, 104, 104, 106, 106, 106, 106, 105, 108, 105, 106, 107, 105, 107, 104, 104, 107, 104, 105, 107, 107, 107, 105, 105, 107, 105, 104, 108, 106, 106, 106, 105, 107, 105, 108, 104, 107]"
  }
 ],
 "info": [
  {
   "2025-03-20 05:01:48": "SpatialDataset : Data schema matchs with the data."
  },
  {
   "2025-03-20 05:01:48": "SpatialDataset : Data points created."
  },
  {
   "2025-03-20 05:01:49": "LgwrKernel : Kernel is initialized."
  },
  {
   "2025-03-20 05:01:49": "LGWR : LGWR model is initialized."
  },
  {
   "2025-03-20 05:01:49": "LgwrOptimizerRL: LgwrOptimizerRL environment is initialized."
  },
  {
   "2025-03-20 05:01:49": "LgwrOptimizerRL: Using AICC as the reward."
  },
  {
   "2025-03-20 05:02:09": "Episode 1 truncated, took 1000 steps, remain 9000 steps, reward: 310.5992736816406."
  },
  {
   "2025-03-20 05:02:34": "Episode 2 truncated, took 1238 steps, remain 7762 steps, reward: 317.5921325683594."
  },
  {
   "2025-03-20 05:02:56": "Episode 3 truncated, took 1091 steps, remain 6671 steps, reward: 316.90972900390625."
  },
  {
   "2025-03-20 05:03:25": "Episode 4 truncated, took 1390 steps, remain 5281 steps, reward: 314.07220458984375."
  },
  {
   "2025-03-20 05:03:44": "Episode 5 truncated, took 1000 steps, remain 4281 steps, reward: 306.3317565917969."
  },
  {
   "2025-03-20 05:04:06": "Episode 6 truncated, took 1001 steps, remain 3280 steps, reward: 306.5982360839844."
  },
  {
   "2025-03-20 05:04:26": "Episode 7 truncated, took 1000 steps, remain 2280 steps, reward: 311.1268005371094."
  },
  {
   "2025-03-20 05:04:47": "Episode 8 truncated, took 1000 steps, remain 1280 steps, reward: 310.4676818847656."
  },
  {
   "2025-03-20 05:05:07": "Episode 9 truncated, took 1000 steps, remain 280 steps, reward: 310.3642272949219."
  },
  {
   "2025-03-20 05:05:18": "PPO: PPO finished training."
  }
 ],
 "matrices": {
  "AIC": null,
  "AICc": null,
  "R-squared": null,
  "R-squared adjusted": null
 }
}